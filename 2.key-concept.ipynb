{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心概念\n",
    "\n",
    " 程序 = 数据结构+算法。\n",
    " TensorFlow程序 = 张量数据结构 + 计算图算法语言\n",
    "\n",
    " **张量** 和 **计算图** 是 TensorFlow 的核心概念。\n",
    "\n",
    " ### 张量数据结构\n",
    "\n",
    " Tensorflow的基本数据结构是张量Tensor。张量即多维数组，Tensorflow的张量和numpy中的array很类似。\n",
    "\n",
    " 从行为特性来看，有两种类型的张量：\n",
    " - 常量constant，常量的值在计算图中不可以被重新赋值\n",
    " - 变量Variable，变量可以在计算图中用assign等算子重新赋值\n",
    "\n",
    "\n",
    " #### 常量张量\n",
    "\n",
    " 张量的数据类型和 `numpy.array` 基本一一对应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "i = tf.constant(1) # tf.int32 类型常量\n",
    "l = tf.constant(1,dtype = tf.int64) # tf.int64 类型常量\n",
    "f = tf.constant(1.23) #tf.float32 类型常量\n",
    "d = tf.constant(3.14,dtype = tf.double) # tf.double 类型常量\n",
    "s = tf.constant(\"hello world\") # tf.string类型常量\n",
    "b = tf.constant(True) #tf.bool类型常量\n",
    "\n",
    "\n",
    "print(tf.int64 == np.int64) \n",
    "print(tf.bool == np.bool)\n",
    "print(tf.double == np.float64)\n",
    "print(tf.string == np.unicode) # tf.string类型和np.unicode类型不等价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同类型的数据可以用不同维度(rank)的张量来表示：\n",
    " - 标量为0维张量，向量为1维张量，矩阵为2维张量\n",
    " - 彩色图像有rgb三个通道，可以表示为3维张量\n",
    " - 视频还有时间维，可以表示为4维张量\n",
    " 可以简单地总结为：有几层中括号，就是多少维的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "scalar = tf.constant(True)  #标量，0维张量\n",
    "\n",
    "print(tf.rank(scalar))\n",
    "print(scalar.numpy().ndim)  # tf.rank的作用和numpy的ndim方法相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "vector = tf.constant([1.0,2.0,3.0,4.0]) #向量，1维张量\n",
    "\n",
    "print(tf.rank(vector))\n",
    "print(np.ndim(vector.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "matrix = tf.constant([[1.0,2.0],[3.0,4.0]]) #矩阵, 2维张量\n",
    "\n",
    "print(tf.rank(matrix).numpy())\n",
    "print(np.dim(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1. 2.]\n",
      "  [3. 4.]]\n",
      "\n",
      " [[5. 6.]\n",
      "  [7. 8.]]], shape=(2, 2, 2), dtype=float32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor3 = tf.constant([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])  # 3维张量\n",
    "print(tensor3)\n",
    "print(tf.rank(tensor3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[1. 1.]\n",
      "   [2. 2.]]\n",
      "\n",
      "  [[3. 3.]\n",
      "   [4. 4.]]]\n",
      "\n",
      "\n",
      " [[[5. 5.]\n",
      "   [6. 6.]]\n",
      "\n",
      "  [[7. 7.]\n",
      "   [8. 8.]]]], shape=(2, 2, 2, 2), dtype=float32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tensor4 = tf.constant([[[[1.0,1.0],[2.0,2.0]],[[3.0,3.0],[4.0,4.0]]],\n",
    "                        [[[5.0,5.0],[6.0,6.0]],[[7.0,7.0],[8.0,8.0]]]])  # 4维张量\n",
    "print(tensor4)\n",
    "print(tf.rank(tensor4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - 可以用`tf.cast`改变张量的数据类型\n",
    " - 可以用`numpy`方法将tensorflow中的张量转化成numpy中的张量\n",
    " - 可以用`shape`方法查看张量的尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dtype: 'int32'> <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "h = tf.constant([123,456],dtype = tf.int32)\n",
    "f = tf.cast(h,tf.float32)\n",
    "print(h.dtype, f.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "print(y.numpy()) #转换成np.array\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe4\\xbd\\xa0\\xe5\\xa5\\xbd \\xe4\\xb8\\x96\\xe7\\x95\\x8c'\n",
      "你好 世界\n"
     ]
    }
   ],
   "source": [
    "u = tf.constant(u\"你好 世界\")\n",
    "print(u.numpy())  \n",
    "print(u.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 变量张量\n",
    "\n",
    " 模型中需要被训练的参数一般被设置成变量Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
      "140601200516624\n",
      "tf.Tensor([2. 3.], shape=(2,), dtype=float32)\n",
      "140601200515616\n"
     ]
    }
   ],
   "source": [
    "# 常量值不可以改变，常量的重新赋值相当于创造新的内存空间\n",
    "c = tf.constant([1.0,2.0])\n",
    "print(c)\n",
    "print(id(c))\n",
    "c = c + tf.constant([1.0,1.0])\n",
    "print(c)\n",
    "print(id(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>\n",
      "140603555167144\n",
      "<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>\n",
      "140603555167144\n"
     ]
    }
   ],
   "source": [
    "# 变量的值可以改变，可以通过assign, assign_add等方法给变量重新赋值\n",
    "v = tf.Variable([1.0,2.0],name = \"v\")\n",
    "print(v)\n",
    "print(id(v))\n",
    "v.assign_add([1.0,1.0])\n",
    "print(v)\n",
    "print(id(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 计算图\n",
    "\n",
    " Tensorflow 有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    " - 在TensorFlow1.0时代，采用的是静态计算图，需要先使用TensorFlow的各种算子创建计算图，然后再开启一个会话Session，显式执行计算图。\n",
    " - 在TensorFlow2.0时代，采用的是动态计算图，即每使用一个算子后，该算子会被动态加入到隐含的默认计算图中立即执行得到结果，而无需开启Session。\n",
    "   - 使用动态计算图即Eager Excution的好处是方便调试程序，它会让TensorFlow代码的表现和Python原生代码的表现一样，写起来就像写numpy一样，各种日志打印，控制流全部都是可以使用的。\n",
    "   - 使用动态计算图的缺点是运行效率相对会低一些。因为使用动态图会有许多次Python进程和TensorFlow的C++进程之间的通信。而静态计算图构建完成之后几乎全部在TensorFlow内核上使用C++          代码执行，效率更高。此外静态图会对计算步骤进行一定的优化，剪去和结果无关的计算步骤。\n",
    " - 如果需要在TensorFlow2.0中使用静态图，可以使用`@tf.function` 装饰器将普通Python函数转换成对应的TensorFlow计算图构建代码。运行该函数就相当于在TensorFlow1.                            0中用Session执行代码。使用`tf.function`构建静态图的方式叫做 `Autograph`\n",
    "\n",
    " #### 计算图简介\n",
    "\n",
    " 计算图由节点(nodes)和线(edges)组成。\n",
    " - 节点表示操作符Operator，或者称之为算子，线表示计算间的依赖。\n",
    " - 实线表示有数据传递依赖，传递的数据即张量。\n",
    " - 虚线通常可以表示控制依赖，即执行先后顺序。\n",
    "\n",
    " ![](./data/strjoin_graph.png)\n",
    "\n",
    " #### 静态计算图\n",
    "\n",
    " 在TensorFlow1.0中，使用静态计算图分两步：\n",
    " - 定义计算图\n",
    " - 在会话中执行计算图\n",
    "\n",
    " ##### TensorFlow 1.0静态计算图范例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello world'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "# 定义计算图\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    #placeholder为占位符，执行会话时候指定填充对象\n",
    "    x = tf.placeholder(name='x', shape=[], dtype=tf.string)  \n",
    "    y = tf.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.string_join([x,y],name = 'join',separator=' ')\n",
    "\n",
    "# 执行计算图\n",
    "with tf.Session(graph = g) as sess:\n",
    "    print(sess.run(fetches = z,feed_dict = {x:\"hello\",y:\"world\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### TensorFlow2.0 怀旧版静态计算图\n",
    "\n",
    " TensorFlow2.0为了确保对老版本tensorflow项目的兼容性，在tf.compat.v1子模块中保留了对TensorFlow1.0那种静态计算图构建风格的支持。\n",
    "\n",
    " 可称之为怀旧版静态计算图，已经不推荐使用了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello world'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g = tf.compat.v1.Graph()\n",
    "with g.as_default():\n",
    "    x = tf.compat.v1.placeholder(name='x', shape=[], dtype=tf.string)\n",
    "    y = tf.compat.v1.placeholder(name='y', shape=[], dtype=tf.string)\n",
    "    z = tf.strings.join([x,y],name = \"join\",separator = \" \")\n",
    "\n",
    "with tf.compat.v1.Session(graph = g) as sess:\n",
    "    # fetches的结果非常像一个函数的返回值，而feed_dict中的占位符相当于函数的参数序列。\n",
    "    result = sess.run(fetches = z,feed_dict = {x:\"hello\",y:\"world\"})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 动态计算图\n",
    "\n",
    " 在TensorFlow2.0中，使用的是动态计算图和Autograph。与TensorFlow1.0中使用静态计算图分两步不同，动态计算图已经不区分计算图的定义和执行了，而是定义后立即执行。因此称之为 `Eager Excution`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# 动态计算图在每个算子处都进行构建，构建后立即执行\n",
    "\n",
    "x = tf.constant(\"hello\")\n",
    "y = tf.constant(\"world\")\n",
    "z = tf.strings.join([x,y],separator=\" \")\n",
    "\n",
    "tf.print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "tf.Tensor(b'hello world', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 可以将动态计算图代码的输入和输出关系封装成函数\n",
    "\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(tf.constant(\"hello\"),tf.constant(\"world\"))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Autograph\n",
    "\n",
    " 动态计算图运行效率相对较低。\n",
    "\n",
    " 可以用 `@tf.function` 装饰器将普通Python函数转换成和TensorFlow1.0对应的静态计算图构建代码。\n",
    "\n",
    " 在TensorFlow1.0中，使用计算图分两步，第一步定义计算图，第二步在会话中执行计算图。\n",
    "\n",
    " 在TensorFlow2.0中，如果采用Autograph的方式使用计算图，第一步定义计算图变成了 **定义函数**，第二步执行计算图变成了**调用函数**。\n",
    "\n",
    " 不需要使用会话了，一些都像原始的Python语法一样自然。\n",
    "\n",
    " 实践中，我们一般会先用动态计算图调试代码，然后在需要提高性能的的地方利用@tf.function切换成Autograph获得更高的效率。\n",
    "\n",
    " 当然，`@tf.function`的使用需要遵循一定的规范，我们后面章节将重点介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "tf.Tensor(b'hello world', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 使用autograph构建静态图\n",
    "\n",
    "@tf.function\n",
    "def strjoin(x,y):\n",
    "    z =  tf.strings.join([x,y],separator = \" \")\n",
    "    tf.print(z)\n",
    "    return z\n",
    "\n",
    "result = strjoin(tf.constant(\"hello\"),tf.constant(\"world\"))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# 创建日志\n",
    "import os\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = os.path.join('data', 'autograph', stamp)\n",
    "\n",
    "## 在 Python3 下建议使用 pathlib 修正各操作系统的路径\n",
    "# from pathlib import Path\n",
    "# stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# logdir = str(Path('./data/autograph/' + stamp))\n",
    "\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "# 开启autograph跟踪\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "\n",
    "# 执行autograph\n",
    "result = strjoin(\"hello\",\"world\")\n",
    "\n",
    "#将计算图信息写入日志\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"autograph\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e77c206b9c34ad13\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e77c206b9c34ad13\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#启动 tensorboard在jupyter中的魔法命令\n",
    "%load_ext tensorboard\n",
    "#启动tensorboard\n",
    "%tensorboard --logdir ./data/autograph/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 自动微分机制\n",
    "\n",
    " 神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情，而深度学习框架可以帮助我们自动地完成这种求梯度运算。\n",
    "\n",
    " Tensorflow一般使用梯度磁带 `tf.GradientTape` 来记录正向运算过程，然后反播磁带自动得到梯度值。\n",
    "\n",
    " 这种利用`tf.GradientTape` 求微分的方法叫做Tensorflow的 **自动微分机制**\n",
    "\n",
    " #### 利用梯度磁带求导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "# f(x) = a*x**2 + b*x + c的导数\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx = tape.gradient(y,x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 对常量张量也可以求导，需要增加watch\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([a,b,c])\n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    \n",
    "dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])\n",
    "print(dy_da)\n",
    "print(dy_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 可以求二阶导数\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape1:   \n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape1.gradient(y,x)   \n",
    "dy2_dx2 = tape2.gradient(dy_dx,x)\n",
    "\n",
    "print(dy2_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-2, 1)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "# 可以在autograph中使用\n",
    "\n",
    "@tf.function\n",
    "def f(x):   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    # 自变量转换成tf.float32\n",
    "    x = tf.cast(x,tf.float32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        y = a*tf.pow(x,2)+b*x+c\n",
    "    dy_dx = tape.gradient(y,x) \n",
    "    \n",
    "    return((dy_dx,y))\n",
    "\n",
    "tf.print(f(tf.constant(0.0)))\n",
    "tf.print(f(tf.constant(1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 利用梯度磁带和优化器求最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0 ; x = 0.999998569\n"
     ]
    }
   ],
   "source": [
    "# 求f(x) = a*x**2 + b*x + c的最小值\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "a = tf.constant(1.0)\n",
    "b = tf.constant(-2.0)\n",
    "c = tf.constant(1.0)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for _ in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = a*tf.pow(x,2) + b*x + c\n",
    "    dy_dx = tape.gradient(y,x)\n",
    "    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "    \n",
    "tf.print(\"y =\",y,\"; x =\",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.99999851\n"
     ]
    }
   ],
   "source": [
    "# 在autograph中完成最小值求解\n",
    "# 使用optimizer.apply_gradients\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "@tf.function\n",
    "def minimizef():\n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    \n",
    "    for _ in tf.range(1000): #注意autograph时使用tf.range(1000)而不是range(1000)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = a*tf.pow(x,2) + b*x + c\n",
    "        dy_dx = tape.gradient(y,x)\n",
    "        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])\n",
    "        \n",
    "    y = a*tf.pow(x,2) + b*x + c\n",
    "    return y\n",
    "\n",
    "tf.print(minimizef())\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.99999851\n"
     ]
    }
   ],
   "source": [
    "# 在autograph中完成最小值求解\n",
    "# 使用optimizer.minimize\n",
    "\n",
    "x = tf.Variable(0.0,name = \"x\",dtype = tf.float32)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   \n",
    "\n",
    "@tf.function\n",
    "def f():   \n",
    "    a = tf.constant(1.0)\n",
    "    b = tf.constant(-2.0)\n",
    "    c = tf.constant(1.0)\n",
    "    y = a*tf.pow(x,2)+b*x+c\n",
    "    return(y)\n",
    "\n",
    "@tf.function\n",
    "def train(epoch):  \n",
    "    for _ in tf.range(epoch):  \n",
    "        optimizer.minimize(f,[x])\n",
    "    return(f())\n",
    "\n",
    "\n",
    "tf.print(train(1000))\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
