{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 低阶API\n",
    "\n",
    "TensorFlow的低阶API主要包括张量操作，计算图和自动微分。如果把模型比作一个房子，那么低阶API就是【模型之砖】。在低阶API层次上，可以把TensorFlow当做一个增强版的numpy来使用。TensorFlow提供的方法比numpy更全面，运算速度更快，如果需要的话，还可以使用GPU进行加速。前面几章我们对低阶API已经有了一个整体的认识，本章我们将重点详细介绍张量操作和Autograph计算图。\n",
    "\n",
    "### 张量的结构操作\n",
    "\n",
    "张量的操作主要包括：\n",
    "\n",
    "- 张量结构操作：张量创建，索引切片，维度变换，合并分割。\n",
    "- 张量数学运算：标量运算，向量运算，矩阵运算。\n",
    "\n",
    "另外我们会介绍张量运算的广播机制。\n",
    "\n",
    "#### 创建张量\n",
    "\n",
    "张量创建的许多方法和numpy中创建array的方法很像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1,2,3],dtype = tf.float32)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5 7 9]\n"
     ]
    }
   ],
   "source": [
    "b = tf.range(1,10,delta = 2)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0.0634343475 0.126868695 ... 6.15313148 6.21656609 6.28]\n"
     ]
    }
   ],
   "source": [
    "c = tf.linspace(0.0,2*3.14,100)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "d = tf.zeros([3,3])\n",
    "tf.print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([3,3])\n",
    "b = tf.zeros_like(a,dtype= tf.float32)\n",
    "tf.print(a)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 5]\n",
      " [5 5]\n",
      " [5 5]]\n"
     ]
    }
   ],
   "source": [
    "b = tf.fill([3,2],5)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65130854 9.01481247 6.30974197 4.34546089 2.9193902]\n"
     ]
    }
   ],
   "source": [
    "#均匀分布随机\n",
    "tf.random.set_seed(1.0)\n",
    "a = tf.random.uniform([5],minval=0,maxval=10)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.403087884 -1.0880208 -0.0630953535]\n",
      " [1.33655667 0.711760104 -0.489286453]\n",
      " [-0.764221311 -1.03724861 -1.25193381]]\n"
     ]
    }
   ],
   "source": [
    "#正态分布随机\n",
    "b = tf.random.normal([3,3],mean=0.0,stddev=1.0)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.457012206 -0.406867266 0.728577793 -0.892977774 -0.369404584]\n",
      " [0.323488563 1.19383323 0.888298929 1.25985587 -1.95951891]\n",
      " [-0.202244401 0.294496894 -0.468728036 1.29494202 1.48142183]\n",
      " [0.0810953453 1.63843894 0.556645 0.977199852 -1.17777884]\n",
      " [1.67368948 0.0647980496 -0.705142677 -0.281972557 0.126546144]]\n"
     ]
    }
   ],
   "source": [
    "#正态分布随机，剔除2倍方差以外数据重新生成\n",
    "c = tf.random.truncated_normal((5,5), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      " \n",
      "[[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "# 特殊矩阵\n",
    "I = tf.eye(3,3) #单位矩阵\n",
    "tf.print(I)\n",
    "tf.print(\" \")\n",
    "t = tf.linalg.diag([1,2,3]) #对角阵\n",
    "tf.print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引切片\n",
    "\n",
    "张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。\n",
    "\n",
    "对于tf.Variable,可以通过索引和切片对部分元素进行修改。\n",
    "\n",
    "对于提取张量的连续子区域，也可以使用tf.slice.\n",
    "\n",
    "此外，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。\n",
    "\n",
    "tf.boolean_mask功能最为强大，它可以实现tf.gather,tf.gather_nd的功能，并且tf.boolean_mask还可以实现布尔索引。\n",
    "\n",
    "如果要通过修改张量的某些元素得到新的张量，可以使用tf.where，tf.scatter_nd。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 7 4 2 9]\n",
      " [9 1 2 4 7]\n",
      " [7 2 7 4 0]\n",
      " [9 6 9 7 2]\n",
      " [3 7 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(3)\n",
    "t = tf.random.uniform([5,5],minval=0,maxval=10,dtype=tf.int32)\n",
    "tf.print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 7 4 2 9]\n"
     ]
    }
   ],
   "source": [
    "#第0行\n",
    "tf.print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 7 0 0 3]\n"
     ]
    }
   ],
   "source": [
    "#倒数第一行\n",
    "tf.print(t[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#第1行第3列\n",
    "tf.print(t[1,3])\n",
    "tf.print(t[1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 1 2 4 7]\n",
      " [7 2 7 4 0]\n",
      " [9 6 9 7 2]]\n",
      "[[9 1 2 4 7]\n",
      " [7 2 7 4 0]\n",
      " [9 6 9 7 2]]\n"
     ]
    }
   ],
   "source": [
    "#第1行至第3行\n",
    "tf.print(t[1:4,:])\n",
    "tf.print(tf.slice(t,[1,0],[3,5])) #tf.slice(input,begin_vector,size_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 2]\n",
      " [7 7]\n",
      " [9 9]]\n"
     ]
    }
   ],
   "source": [
    "#第1行至最后一行，第0列到最后一列每隔两列取一列\n",
    "tf.print(t[1:4,:4:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "#对变量来说，还可以使用索引和切片修改部分元素\n",
    "x = tf.Variable([[1,2],[3,4]],dtype = tf.float32)\n",
    "x[1,:].assign(tf.constant([0.0,0.0]))\n",
    "tf.print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[7 3 9]\n",
      "  [9 0 7]\n",
      "  [9 6 7]]\n",
      "\n",
      " [[1 3 3]\n",
      "  [0 8 1]\n",
      "  [3 1 0]]\n",
      "\n",
      " [[4 0 6]\n",
      "  [6 2 2]\n",
      "  [7 9 5]]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform([3,3,3],minval=0,maxval=10,dtype=tf.int32)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 6]\n",
      " [3 8 1]\n",
      " [0 2 9]]\n"
     ]
    }
   ],
   "source": [
    "#省略号可以表示多个冒号\n",
    "tf.print(a[...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上切片方式相对规则，对于不规则的切片提取,可以使用tf.gather,tf.gather_nd,tf.boolean_mask。\n",
    "\n",
    "考虑班级成绩册的例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52 82 66 ... 17 86 14]\n",
      "  [8 36 94 ... 13 78 41]\n",
      "  [77 53 51 ... 22 91 56]\n",
      "  ...\n",
      "  [11 19 26 ... 89 86 68]\n",
      "  [60 72 0 ... 11 26 15]\n",
      "  [24 99 38 ... 97 44 74]]\n",
      "\n",
      " [[79 73 73 ... 35 3 81]\n",
      "  [83 36 31 ... 75 38 85]\n",
      "  [54 26 67 ... 60 68 98]\n",
      "  ...\n",
      "  [20 5 18 ... 32 45 3]\n",
      "  [72 52 81 ... 88 41 20]\n",
      "  [0 21 89 ... 53 10 90]]\n",
      "\n",
      " [[52 80 22 ... 29 25 60]\n",
      "  [78 71 54 ... 43 98 81]\n",
      "  [21 66 53 ... 97 75 77]\n",
      "  ...\n",
      "  [6 74 3 ... 53 65 43]\n",
      "  [98 36 72 ... 33 36 81]\n",
      "  [61 78 70 ... 7 59 21]]\n",
      "\n",
      " [[56 57 45 ... 23 15 3]\n",
      "  [35 8 82 ... 11 59 97]\n",
      "  [44 6 99 ... 81 60 27]\n",
      "  ...\n",
      "  [76 26 35 ... 51 8 17]\n",
      "  [33 52 53 ... 78 37 31]\n",
      "  [71 27 44 ... 0 52 16]]]\n"
     ]
    }
   ],
   "source": [
    "scores = tf.random.uniform((4,10,7),minval=0,maxval=100,dtype=tf.int32)\n",
    "tf.print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52 82 66 ... 17 86 14]\n",
      "  [24 80 70 ... 72 63 96]\n",
      "  [24 99 38 ... 97 44 74]]\n",
      "\n",
      " [[79 73 73 ... 35 3 81]\n",
      "  [46 10 94 ... 23 18 92]\n",
      "  [0 21 89 ... 53 10 90]]\n",
      "\n",
      " [[52 80 22 ... 29 25 60]\n",
      "  [19 12 23 ... 87 86 25]\n",
      "  [61 78 70 ... 7 59 21]]\n",
      "\n",
      " [[56 57 45 ... 23 15 3]\n",
      "  [6 41 79 ... 97 43 13]\n",
      "  [71 27 44 ... 0 52 16]]]\n"
     ]
    }
   ],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩\n",
    "p = tf.gather(scores,[0,5,9],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[82 55 14]\n",
      "  [80 46 96]\n",
      "  [99 58 74]]\n",
      "\n",
      " [[73 48 81]\n",
      "  [10 38 92]\n",
      "  [21 86 90]]\n",
      "\n",
      " [[80 57 60]\n",
      "  [12 34 25]\n",
      "  [78 71 21]]\n",
      "\n",
      " [[57 75 3]\n",
      "  [41 47 13]\n",
      "  [27 96 16]]]\n"
     ]
    }
   ],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩\n",
    "q = tf.gather(tf.gather(scores,[0,5,9],axis=1),[1,3,6],axis=2)\n",
    "tf.print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=int32, numpy=\n",
       "array([[52, 82, 66, 55, 17, 86, 14],\n",
       "       [99, 94, 46, 70,  1, 63, 41],\n",
       "       [46, 83, 70, 80, 90, 85, 17]], dtype=int32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩\n",
    "#indices的长度为采样样本的个数，每个元素为采样位置的坐标\n",
    "s = tf.gather_nd(scores,indices = [(0,0),(2,4),(3,6)])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上tf.gather和tf.gather_nd的功能也可以用tf.boolean_mask来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52 82 66 ... 17 86 14]\n",
      "  [24 80 70 ... 72 63 96]\n",
      "  [24 99 38 ... 97 44 74]]\n",
      "\n",
      " [[79 73 73 ... 35 3 81]\n",
      "  [46 10 94 ... 23 18 92]\n",
      "  [0 21 89 ... 53 10 90]]\n",
      "\n",
      " [[52 80 22 ... 29 25 60]\n",
      "  [19 12 23 ... 87 86 25]\n",
      "  [61 78 70 ... 7 59 21]]\n",
      "\n",
      " [[56 57 45 ... 23 15 3]\n",
      "  [6 41 79 ... 97 43 13]\n",
      "  [71 27 44 ... 0 52 16]]]\n"
     ]
    }
   ],
   "source": [
    "#抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩\n",
    "p = tf.boolean_mask(scores,[True,False,False,False,False,\n",
    "                            True,False,False,False,True],axis=1)\n",
    "tf.print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[52 82 66 ... 17 86 14]\n",
      " [99 94 46 ... 1 63 41]\n",
      " [46 83 70 ... 90 85 17]]\n"
     ]
    }
   ],
   "source": [
    "#抽取第0个班级第0个学生，第2个班级的第4个学生，第3个班级的第6个学生的全部成绩\n",
    "s = tf.boolean_mask(scores,\n",
    "    [[True,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,False,False,False,False],\n",
    "     [False,False,False,False,True,False,False,False,False,False],\n",
    "     [False,False,False,False,False,False,True,False,False,False]])\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 1 -1]\n",
      " [2 2 -2]\n",
      " [3 -3 3]] \n",
      "\n",
      "[-1 -1 -2 -3] \n",
      "\n",
      "[-1 -1 -2 -3]\n"
     ]
    }
   ],
   "source": [
    "#利用tf.boolean_mask可以实现布尔索引\n",
    "\n",
    "#找到矩阵中小于0的元素\n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "tf.print(c,\"\\n\")\n",
    "\n",
    "tf.print(tf.boolean_mask(c,c<0),\"\\n\") \n",
    "tf.print(c[c<0]) #布尔索引，为boolean_mask的语法糖形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。\n",
    "\n",
    "如果要通过修改张量的部分元素值得到新的张量，可以使用tf.where和tf.scatter_nd。\n",
    "\n",
    "tf.where可以理解为if的张量版本，此外它还可以用于找到满足条件的所有元素的位置坐标。\n",
    "\n",
    "tf.scatter_nd的作用和tf.gather_nd有些相反，tf.gather_nd用于收集张量的给定位置的元素，\n",
    "\n",
    "而tf.scatter_nd可以将某些值插入到一个给定shape的全0的张量的指定位置处。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[nan,  1., nan],\n",
       "       [ 2.,  2., nan],\n",
       "       [ 3., nan,  3.]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找到张量中小于0的元素,将其换成np.nan得到新的张量\n",
    "#tf.where和np.where作用类似，可以理解为if的张量版本\n",
    "\n",
    "c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)\n",
    "d = tf.where(c<0,tf.fill(c.shape,np.nan),c) \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
       "array([[0, 0],\n",
       "       [0, 2],\n",
       "       [1, 2],\n",
       "       [2, 1]])>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果where只有一个参数，将返回所有满足条件的位置坐标\n",
    "indices = tf.where(c<0)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.,  1., -1.],\n",
       "       [ 2.,  2., -2.],\n",
       "       [ 3.,  0.,  3.]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将张量的第[0,0]和[2,1]两个位置元素替换为0得到新的张量\n",
    "d = c - tf.scatter_nd([[0,0],[2,1]],[c[0,0],c[2,1]],c.shape)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-1.,  0., -1.],\n",
       "       [ 0.,  0., -2.],\n",
       "       [ 0., -3.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scatter_nd的作用和gather_nd有些相反\n",
    "#可以将某些值插入到一个给定shape的全0的张量的指定位置处。\n",
    "indices = tf.where(c<0)\n",
    "tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 维度变换\n",
    "维度变换相关函数主要有 tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.\n",
    "\n",
    "tf.reshape 可以改变张量的形状。\n",
    "\n",
    "tf.squeeze 可以减少维度。\n",
    "\n",
    "tf.expand_dims 可以增加维度。\n",
    "\n",
    "tf.transpose 可以交换维度。\n",
    "\n",
    "tf.reshape可以改变张量的形状，但是其本质上不会改变张量元素的存储顺序，所以，该操作实际上非常迅速，并且是可逆的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([1, 3, 3, 2])\n",
      "[[[[100 44]\n",
      "   [181 14]\n",
      "   [90 53]]\n",
      "\n",
      "  [[205 141]\n",
      "   [14 24]\n",
      "   [239 46]]\n",
      "\n",
      "  [[225 174]\n",
      "   [212 78]\n",
      "   [14 144]]]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=[1,3,3,2],\n",
    "                      minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([3, 6])\n",
      "[[100 44 181 14 90 53]\n",
      " [205 141 14 24 239 46]\n",
      " [225 174 212 78 14 144]]\n"
     ]
    }
   ],
   "source": [
    "# 改成 （3,6）形状的张量\n",
    "b = tf.reshape(a,[3,6])\n",
    "tf.print(b.shape)\n",
    "tf.print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[100 44]\n",
      "   [181 14]\n",
      "   [90 53]]\n",
      "\n",
      "  [[205 141]\n",
      "   [14 24]\n",
      "   [239 46]]\n",
      "\n",
      "  [[225 174]\n",
      "   [212 78]\n",
      "   [14 144]]]]\n"
     ]
    }
   ],
   "source": [
    "# 改回成 [1,3,3,2] 形状的张量\n",
    "c = tf.reshape(b,[1,3,3,2])\n",
    "tf.print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果张量在某个维度上只有一个元素，利用tf.squeeze可以消除这个维度。\n",
    "\n",
    "和tf.reshape相似，它本质上不会改变张量元素的存储顺序。\n",
    "\n",
    "张量的各个元素在内存中是线性存储的，其一般规律是，同一层级中的相邻元素的物理地址也相邻。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([3, 3, 2])\n",
      "[[[100 44]\n",
      "  [181 14]\n",
      "  [90 53]]\n",
      "\n",
      " [[205 141]\n",
      "  [14 24]\n",
      "  [239 46]]\n",
      "\n",
      " [[225 174]\n",
      "  [212 78]\n",
      "  [14 144]]]\n"
     ]
    }
   ],
   "source": [
    "s = tf.squeeze(a)\n",
    "tf.print(s.shape)\n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 3, 2), dtype=int32, numpy=\n",
       "array([[[[100,  44],\n",
       "         [181,  14],\n",
       "         [ 90,  53]],\n",
       "\n",
       "        [[205, 141],\n",
       "         [ 14,  24],\n",
       "         [239,  46]],\n",
       "\n",
       "        [[225, 174],\n",
       "         [212,  78],\n",
       "         [ 14, 144]]]], dtype=int32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf.expand_dims(s,axis=0) #在第0维插入长度为1的一个维度\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.transpose可以交换张量的维度，与tf.reshape不同，它会改变张量元素的存储顺序。\n",
    "\n",
    "tf.transpose常用于图片存储格式的变换上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([100, 600, 600, 4])\n",
      "TensorShape([4, 600, 600, 100])\n"
     ]
    }
   ],
   "source": [
    "# Batch,Height,Width,Channel\n",
    "a = tf.random.uniform(shape=[100,600,600,4],minval=0,maxval=255,dtype=tf.int32)\n",
    "tf.print(a.shape)\n",
    "\n",
    "# 转换成 Channel,Height,Width,Batch\n",
    "s= tf.transpose(a,perm=[3,1,2,0])\n",
    "tf.print(s.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合并分割\n",
    "和numpy类似，可以用tf.concat和tf.stack方法对多个张量进行合并，可以用tf.split方法把一个张量分割成多个张量。\n",
    "\n",
    "tf.concat和tf.stack有略微的区别，tf.concat是连接，不会增加维度，而tf.stack是堆叠，会增加维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 2), dtype=float32, numpy=\n",
       "array([[ 1.,  2.],\n",
       "       [ 3.,  4.],\n",
       "       [ 5.,  6.],\n",
       "       [ 7.,  8.],\n",
       "       [ 9., 10.],\n",
       "       [11., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "\n",
    "tf.concat([a,b,c],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n",
       "array([[ 1.,  2.,  5.,  6.,  9., 10.],\n",
       "       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat([a,b,c],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=\n",
       "array([[[ 1.,  2.],\n",
       "        [ 3.,  4.]],\n",
       "\n",
       "       [[ 5.,  6.],\n",
       "        [ 7.,  8.]],\n",
       "\n",
       "       [[ 9., 10.],\n",
       "        [11., 12.]]], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
       "array([[[ 1.,  2.],\n",
       "        [ 5.,  6.],\n",
       "        [ 9., 10.]],\n",
       "\n",
       "       [[ 3.,  4.],\n",
       "        [ 7.,  8.],\n",
       "        [11., 12.]]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([a,b,c],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1.0,2.0],[3.0,4.0]])\n",
    "b = tf.constant([[5.0,6.0],[7.0,8.0]])\n",
    "c = tf.constant([[9.0,10.0],[11.0,12.0]])\n",
    "\n",
    "c = tf.concat([a,b,c],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.split是tf.concat的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 2.],\n",
       "        [3., 4.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[5., 6.],\n",
       "        [7., 8.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 9., 10.],\n",
       "        [11., 12.]], dtype=float32)>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.split(value,num_or_size_splits,axis)\n",
    "tf.split(c,3,axis = 0)  #指定分割份数，平均分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[1., 2.],\n",
       "        [3., 4.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[5., 6.],\n",
       "        [7., 8.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 9., 10.],\n",
       "        [11., 12.]], dtype=float32)>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.split(c,[2,2,2],axis = 0) #指定每份的记录数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 张量的数学运算\n",
    "张量的操作主要包括张量的结构操作和张量的数学运算。\n",
    "\n",
    "张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。\n",
    "\n",
    "张量数学运算主要有：标量运算，向量运算，矩阵运算。另外我们会介绍张量运算的广播机制。\n",
    "\n",
    "本篇我们介绍张量的数学运算。\n",
    "\n",
    "#### 标量运算\n",
    "张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。\n",
    "\n",
    "加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。\n",
    "\n",
    "标量运算符的特点是对张量实施逐元素运算。\n",
    "\n",
    "有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。\n",
    "\n",
    "许多标量运算符都在 tf.math模块下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 6.,  8.],\n",
       "       [ 4., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1.0,2],[-3,4.0]])\n",
    "b = tf.constant([[5.0,6],[7.0,8.0]])\n",
    "a+b  #运算符重载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ -4.,  -4.],\n",
       "       [-10.,  -4.]], dtype=float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a-b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[  5.,  12.],\n",
       "       [-21.,  32.]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.2       ,  0.33333334],\n",
       "       [-0.42857143,  0.5       ]], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 1.,  4.],\n",
       "       [ 9., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.       , 1.4142135],\n",
       "       [      nan, 2.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a**(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 1.,  2.],\n",
       "       [-0.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a%3 #mod的运算符重载，等价于m = tf.math.mod(a,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.,  0.],\n",
       "       [-1.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a//3  #地板除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
       "array([[False,  True],\n",
       "       [False,  True]])>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a>=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
       "array([[False,  True],\n",
       "       [False, False]])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a>=2)&(a<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
       "array([[ True,  True],\n",
       "       [ True,  True]])>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a>=2)|(a<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=bool, numpy=\n",
       "array([[False, False],\n",
       "       [False, False]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a==5 #tf.equal(a,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.       , 1.4142135],\n",
       "       [      nan, 2.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sqrt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1.0,8.0])\n",
    "b = tf.constant([5.0,6.0])\n",
    "c = tf.constant([6.0,7.0])\n",
    "tf.add_n([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 8]\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.maximum(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 6]\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.minimum(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 -3]\n",
      "[2 -3]\n",
      "[3 -2]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([2.6,-2.7])\n",
    "\n",
    "tf.print(tf.math.round(x)) #保留整数部分，四舍五入\n",
    "tf.print(tf.math.floor(x)) #保留整数部分，向下归整\n",
    "tf.print(tf.math.ceil(x))  #保留整数部分，向上归整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9 -0.8 1 -1 0.7]\n",
      "[0.0264732055 -0.0235317405 2.94146752 -0.588293493 0.0205902718]\n"
     ]
    }
   ],
   "source": [
    "# 幅值裁剪\n",
    "x = tf.constant([0.9,-0.8,100.0,-20.0,0.7])\n",
    "y = tf.clip_by_value(x,clip_value_min=-1,clip_value_max=1)\n",
    "z = tf.clip_by_norm(x,clip_norm = 3)\n",
    "tf.print(y)\n",
    "tf.print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量运算\n",
    "\n",
    "向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。 许多向量运算符都以reduce开头。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "5\n",
      "9\n",
      "1\n",
      "362880\n"
     ]
    }
   ],
   "source": [
    "#向量reduce\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.reduce_sum(a))\n",
    "tf.print(tf.reduce_mean(a))\n",
    "tf.print(tf.reduce_max(a))\n",
    "tf.print(tf.reduce_min(a))\n",
    "tf.print(tf.reduce_prod(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [15]\n",
      " [24]]\n",
      "[[12 15 18]]\n"
     ]
    }
   ],
   "source": [
    "#张量指定维度进行reduce\n",
    "b = tf.reshape(a,(3,3))\n",
    "tf.print(tf.reduce_sum(b, axis=1, keepdims=True))\n",
    "tf.print(tf.reduce_sum(b, axis=0, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#bool类型的reduce\n",
    "p = tf.constant([True,False,False])\n",
    "q = tf.constant([False,False,True])\n",
    "tf.print(tf.reduce_all(p))\n",
    "tf.print(tf.reduce_any(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "#利用tf.foldr实现tf.reduce_sum\n",
    "s = tf.foldr(lambda a,b:a+b,tf.range(10)) \n",
    "tf.print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#arg最大最小值索引\n",
    "a = tf.range(1,10)\n",
    "tf.print(tf.argmax(a))\n",
    "tf.print(tf.argmin(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 7 5]\n",
      "[5 2 3]\n"
     ]
    }
   ],
   "source": [
    "#tf.math.top_k可以用于对张量排序\n",
    "a = tf.constant([1,3,7,5,4,8])\n",
    "\n",
    "values,indices = tf.math.top_k(a,3,sorted=True)\n",
    "tf.print(values)\n",
    "tf.print(indices)\n",
    "\n",
    "#利用tf.math.top_k可以在TensorFlow中实现KNN算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵运算\n",
    "矩阵必须是二维的。类似tf.constant([1,2,3])这样的不是矩阵。\n",
    "\n",
    "矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。\n",
    "\n",
    "除了一些常用的运算外，大部分和矩阵有关的运算都在tf.linalg子包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2, 4],\n",
       "       [6, 8]], dtype=int32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵乘法\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[2,0],[0,2]])\n",
    "a@b  #等价于tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 3],\n",
       "       [2, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵转置\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "tf.transpose(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-2. ,  1. ],\n",
       "       [ 1.5, -0.5]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵逆，必须为tf.float32或tf.double类型\n",
    "a = tf.constant([[1.0,2],[3,4]],dtype = tf.float32)\n",
    "tf.linalg.inv(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵求trace\n",
    "a = tf.constant([[1.0,2],[3,4]],dtype = tf.float32)\n",
    "tf.linalg.trace(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.477226>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵求范数\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵行列式\n",
    "a = tf.constant([[1.0,2],[3,4]])\n",
    "tf.linalg.det(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=complex64, numpy=array([2.4999995+2.7838817j, 2.5      -2.783882j ], dtype=complex64)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#矩阵特征值\n",
    "a = tf.constant([[1.0,2],[-5,4]])\n",
    "tf.linalg.eigvals(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.316227794 -0.948683321]\n",
      " [-0.948683321 0.316227734]]\n",
      "[[-3.1622777 -4.4271884]\n",
      " [0 -0.632455349]]\n",
      "[[1.00000012 1.99999976]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵QR分解, 将一个方阵分解为一个正交矩阵q和上三角矩阵r\n",
    "#QR分解实际上是对矩阵a实施Schmidt正交化得到q\n",
    "\n",
    "a = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)\n",
    "q,r = tf.linalg.qr(a)\n",
    "tf.print(q)\n",
    "tf.print(r)\n",
    "tf.print(q@r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.229847908 0.883461416]\n",
      " [-0.524744868 0.240781903]\n",
      " [-0.819642 -0.401895702]] \n",
      "\n",
      "[9.52551937 0.514301538] \n",
      "\n",
      "[[-0.619629383 -0.784894526]\n",
      " [-0.784894526 0.619629383]] \n",
      "\n",
      "[[1.00000036 2.00000262]\n",
      " [3.00000024 4.00000143]\n",
      " [5.00000048 6.00000143]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵svd分解\n",
    "#svd分解可以将任意一个矩阵分解为一个正交矩阵u,一个对角阵s和一个正交矩阵v.t()的乘积\n",
    "#svd常用于矩阵压缩和降维\n",
    "\n",
    "a  = tf.constant([[1.0,2.0],[3.0,4.0],[5.0,6.0]], dtype = tf.float32)\n",
    "s,u,v = tf.linalg.svd(a)\n",
    "tf.print(u,\"\\n\")\n",
    "tf.print(s,\"\\n\")\n",
    "tf.print(v,\"\\n\")\n",
    "tf.print(u@tf.linalg.diag(s)@tf.transpose(v))\n",
    "\n",
    "#利用svd分解可以在TensorFlow中实现主成分分析降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 广播机制\n",
    "TensorFlow的广播规则和numpy是一样的:\n",
    "\n",
    "1. 如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。\n",
    "2. 如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。\n",
    "3. 如果两个张量在所有维度上都是相容的，它们就能使用广播。\n",
    "4. 广播之后，每个维度的长度将取两个张量在该维度长度的较大值。\n",
    "5. 在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。\n",
    "\n",
    "tf.broadcast_to 以显式的方式按照广播机制扩展张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [2, 3, 4],\n",
       "       [3, 4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])\n",
    "b + a  #等价于 b + tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]], dtype=int32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.broadcast_to(a,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 3])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算广播后计算结果的形状，静态形状，TensorShape类型参数\n",
    "tf.broadcast_static_shape(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算广播后计算结果的形状，动态形状，Tensor类型参数\n",
    "c = tf.constant([1,2,3])\n",
    "d = tf.constant([[1],[2],[3]])\n",
    "tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[2, 3, 4],\n",
       "       [3, 4, 5],\n",
       "       [4, 5, 6]], dtype=int32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#广播效果\n",
    "c+d #等价于 tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph的使用规范\n",
    "\n",
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    "TensorFlow 2.0主要使用的是动态计算图和Autograph。\n",
    "\n",
    "动态计算图易于调试，编码效率较高，但执行效率偏低。\n",
    "\n",
    "静态计算图执行效率很高，但较难调试。\n",
    "\n",
    "而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。\n",
    "\n",
    "当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。\n",
    "\n",
    "我们将着重介绍Autograph的编码规范和Autograph转换成静态图的原理。\n",
    "\n",
    "并介绍使用tf.Module来更好地构建Autograph。\n",
    "\n",
    "本篇我们介绍使用Autograph的编码规范。\n",
    "\n",
    "#### Autograph编码规范总结\n",
    "\n",
    "\n",
    "* 被 `@tf.function` 修饰的函数应尽可能使用TensorFlow中的函数而不是Python中的其他函数。例如使用 `tf.print`而不是print，使用 `tf.range` 而不是range，使用 `tf.constant(True)` 而不是True.\n",
    "\n",
    "* 避免在 `@tf.function` 修饰的函数内部定义 `tf.Variable`.\n",
    "\n",
    "* 被 `@tf.function`修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。\n",
    "\n",
    "#### Autograph编码规范解析\n",
    "\n",
    "\n",
    " **被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def np_random():\n",
    "    a = np.random.randn(3,3)\n",
    "    tf.print(a)\n",
    "\n",
    "@tf.function\n",
    "def tf_random():\n",
    "    a = tf.random.normal((3,3))\n",
    "    tf.print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.37185999, -0.66276917,  0.30457205],\n",
      "       [-0.02371704,  1.47153832, -1.94096433],\n",
      "       [ 0.68466123,  1.68395685,  1.67622844]])\n",
      "array([[ 0.37185999, -0.66276917,  0.30457205],\n",
      "       [-0.02371704,  1.47153832, -1.94096433],\n",
      "       [ 0.68466123,  1.68395685,  1.67622844]])\n"
     ]
    }
   ],
   "source": [
    "#np_random每次执行都是一样的结果。\n",
    "np_random()\n",
    "np_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.86471796 0.802644968 -0.443217158]\n",
      " [0.13816683 0.983881354 -1.11580396]\n",
      " [0.406377852 0.282590836 0.813724637]]\n",
      "[[0.799207 -0.559536636 -2.13951492]\n",
      " [-0.519267499 0.746292412 -1.14511716]\n",
      " [2.00111032 -0.747018635 -1.17452967]]\n"
     ]
    }
   ],
   "source": [
    "#tf_random每次执行都会有重新生成随机数。\n",
    "tf_random()\n",
    "tf_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**避免在@tf.function修饰的函数内部定义tf.Variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 避免在@tf.function修饰的函数内部定义tf.Variable.\n",
    "\n",
    "x = tf.Variable(1.0,dtype=tf.float32)\n",
    "@tf.function\n",
    "def outer_var():\n",
    "    x.assign_add(1.0)\n",
    "    tf.print(x)\n",
    "    return(x)\n",
    "\n",
    "outer_var()\n",
    "outer_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def inner_var():\n",
    "    x = tf.Variable(1.0,dtype = tf.float32)\n",
    "    x.assign_add(1.0)\n",
    "    tf.print(x)\n",
    "    return(x)\n",
    "\n",
    "#执行将报错\n",
    "#inner_var()\n",
    "#inner_var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=5.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "tensor_list = []\n",
    "\n",
    "#@tf.function #加上这一行切换成Autograph结果将不符合预期！！！\n",
    "def append_tensor(x):\n",
    "    tensor_list.append(x)\n",
    "    return tensor_list\n",
    "\n",
    "append_tensor(tf.constant(5.0))\n",
    "append_tensor(tf.constant(6.0))\n",
    "print(tensor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'x:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "tensor_list = []\n",
    "\n",
    "@tf.function #加上这一行切换成Autograph结果将不符合预期！！！\n",
    "def append_tensor(x):\n",
    "    tensor_list.append(x)\n",
    "    return tensor_list\n",
    "\n",
    "\n",
    "append_tensor(tf.constant(5.0))\n",
    "append_tensor(tf.constant(6.0))\n",
    "print(tensor_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGraph的机制原理\n",
    "\n",
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    "TensorFlow 2.0主要使用的是动态计算图和Autograph。\n",
    "\n",
    "动态计算图易于调试，编码效率较高，但执行效率偏低。\n",
    "\n",
    "静态计算图执行效率很高，但较难调试。\n",
    "\n",
    "而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。\n",
    "\n",
    "当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。\n",
    "\n",
    "我们会介绍Autograph的编码规范和Autograph转换成静态图的原理。\n",
    "\n",
    "并介绍使用tf.Module来更好地构建Autograph。\n",
    "\n",
    "上篇我们介绍了Autograph的编码规范，本篇我们介绍Autograph的机制原理。\n",
    "\n",
    "#### Autograph的机制原理\n",
    "\n",
    "\n",
    "**当我们使用@tf.function装饰一个函数的时候，后面到底发生了什么呢？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "@tf.function(autograph=True)\n",
    "def myadd(a,b):\n",
    "    for i in tf.range(3):\n",
    "        tf.print(i)\n",
    "    c = a+b\n",
    "    print(\"tracing\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后面什么都没有发生。仅仅是在Python堆栈中记录了这样一个函数的签名。\n",
    "\n",
    "**当我们第一次调用这个被@tf.function装饰的函数时，后面到底发生了什么？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracing\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'helloworld'>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(tf.constant(\"hello\"),tf.constant(\"world\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发生了2件事情，\n",
    "\n",
    "第一件事情是创建计算图。\n",
    "\n",
    "即创建一个静态计算图，跟踪执行一遍函数体中的Python代码，确定各个变量的Tensor类型，并根据执行顺序将算子添加到计算图中。\n",
    "在这个过程中，如果开启了autograph=True(默认开启),会将Python控制流转换成TensorFlow图内控制流。\n",
    "主要是将if语句转换成 tf.cond算子表达，将while和for循环语句转换成tf.while_loop算子表达，并在必要的时候添加\n",
    "tf.control_dependencies指定执行顺序依赖关系。\n",
    "\n",
    "相当于在 tensorflow1.0执行了类似下面的语句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.placeholder(shape=[],dtype=tf.string)\n",
    "    b = tf.placeholder(shape=[],dtype=tf.string)\n",
    "    cond = lambda i: i<tf.constant(3)\n",
    "    def body(i):\n",
    "        tf.print(i)\n",
    "        return(i+1)\n",
    "    loop = tf.while_loop(cond,body,loop_vars=[0])\n",
    "    loop\n",
    "    with tf.control_dependencies(loop):\n",
    "        c = tf.strings.join([a,b])\n",
    "    print(\"tracing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二件事情是执行计算图。\n",
    "\n",
    "相当于在 tensorflow1.0中执行了下面的语句："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(c,feed_dict={a:tf.constant(\"hello\"),b:tf.constant(\"world\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们先看到的是第一个步骤的结果：即Python调用标准输出流打印\"tracing\"语句。\n",
    "\n",
    "然后看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。\n",
    "\n",
    "**当我们再次用相同的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'goodmorning'>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(tf.constant(\"good\"),tf.constant(\"morning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只会发生一件事情，那就是上面步骤的第二步，执行计算图。\n",
    "\n",
    "所以这一次我们没有看到打印\"tracing\"的结果。\n",
    "\n",
    "\n",
    "**当我们再次用不同的的输入参数类型调用这个被@tf.function装饰的函数时，后面到底发生了什么？**\n",
    "\n",
    "例如我们写下如下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracing\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(tf.constant(1),tf.constant(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于输入参数的类型已经发生变化，已经创建的计算图不能够再次使用。\n",
    "\n",
    "需要重新做2件事情：创建新的计算图、执行计算图。\n",
    "\n",
    "所以我们又会先看到的是第一个步骤的结果：即Python调用标准输出流打印\"tracing\"语句。\n",
    "\n",
    "然后再看到第二个步骤的结果：TensorFlow调用标准输出流打印1,2,3。\n",
    "\n",
    "\n",
    "**需要注意的是，如果调用被@tf.function装饰的函数时输入的参数不是Tensor类型，则每次都会重新创建计算图。**\n",
    "\n",
    "例如我们写下如下代码。两次都会重新创建计算图。因此，一般建议调用@tf.function时应传入Tensor类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tracing\n",
      "0\n",
      "1\n",
      "2\n",
      "tracing\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'goodmorning'>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myadd(\"hello\",\"world\")\n",
    "myadd(\"good\",\"morning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重新理解Autograph的编码规范\n",
    "\n",
    "\n",
    "了解了以上Autograph的机制原理，我们也就能够理解Autograph编码规范的3条建议了。\n",
    "\n",
    "1. 被@tf.function修饰的函数应尽量使用TensorFlow中的函数而不是Python中的其他函数。例如使用tf.print而不是print.\n",
    "\n",
    "解释：Python中的函数仅仅会在跟踪执行函数以创建静态图的阶段使用，普通Python函数是无法嵌入到静态计算图中的，所以\n",
    "在计算图构建好之后再次调用的时候，这些Python函数并没有被计算，而TensorFlow中的函数则可以嵌入到计算图中。使用普通的Python函数会导致\n",
    "被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。\n",
    "\n",
    "2. 避免在@tf.function修饰的函数内部定义tf.Variable. \n",
    "\n",
    "解释：如果函数内部定义了tf.Variable,那么在【eager执行】时，这种创建tf.Variable的行为在每次函数调用时候都会发生。但是在【静态图执行】时，这种创建tf.Variable的行为只会发生在第一步跟踪Python代码逻辑创建计算图时，这会导致被@tf.function修饰前【eager执行】和被@tf.function修饰后【静态图执行】的输出不一致。实际上，TensorFlow在这种情况下一般会报错。\n",
    "\n",
    "3. 被@tf.function修饰的函数不可修改该函数外部的Python列表或字典等数据结构变量。\n",
    "\n",
    "解释：静态计算图是被编译成C++代码在TensorFlow内核中执行的。Python中的列表和字典等数据结构变量是无法嵌入到计算图中，它们仅仅能够在创建计算图时被读取，在执行计算图时是无法修改Python中的列表或字典这样的数据结构变量的。\n",
    "\n",
    "### AutoGraph和tf.Module\n",
    "\n",
    "\n",
    "有三种计算图的构建方式：静态计算图，动态计算图，以及Autograph。\n",
    "\n",
    "TensorFlow 2.0主要使用的是动态计算图和Autograph。\n",
    "\n",
    "动态计算图易于调试，编码效率较高，但执行效率偏低。\n",
    "\n",
    "静态计算图执行效率很高，但较难调试。\n",
    "\n",
    "而Autograph机制可以将动态图转换成静态计算图，兼收执行效率和编码效率之利。\n",
    "\n",
    "当然Autograph机制能够转换的代码并不是没有任何约束的，有一些编码规范需要遵循，否则可能会转换失败或者不符合预期。\n",
    "\n",
    "前面我们介绍了Autograph的编码规范和Autograph转换成静态图的原理。\n",
    "\n",
    "本篇我们介绍使用tf.Module来更好地构建Autograph。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Autograph和tf.Module概述\n",
    "\n",
    "\n",
    "前面在介绍Autograph的编码规范时提到构建Autograph时应该避免在@tf.function修饰的函数内部定义tf.Variable. \n",
    "\n",
    "但是如果在函数外部定义tf.Variable的话，又会显得这个函数有外部变量依赖，封装不够完美。\n",
    "\n",
    "一种简单的思路是定义一个类，并将相关的tf.Variable创建放在类的初始化方法中。而将函数的逻辑放在其他方法中。\n",
    "\n",
    "这样一顿猛如虎的操作之后，我们会觉得一切都如同人法地地法天天法道道法自然般的自然。\n",
    "\n",
    "惊喜的是，TensorFlow提供了一个基类tf.Module，通过继承它构建子类，我们不仅可以获得以上的自然而然，而且可以非常方便地管理变量，还可以非常方便地管理它引用的其它Module，最重要的是，我们能够利用tf.saved_model保存模型并实现跨平台部署使用。\n",
    "\n",
    "实际上，tf.keras.models.Model,tf.keras.layers.Layer 都是继承自tf.Module的，提供了方便的变量管理和所引用的子模块管理的功能。\n",
    "\n",
    "**因此，利用tf.Module提供的封装，再结合TensoFlow丰富的低阶API，实际上我们能够基于TensorFlow开发任意机器学习模型(而非仅仅是神经网络模型)，并实现跨平台部署使用。**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 应用tf.Module封装Autograph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "x = tf.Variable(1.0,dtype=tf.float32)\n",
    "\n",
    "#在tf.function中用input_signature限定输入张量的签名类型：shape和dtype\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    \n",
    "def add_print(a):\n",
    "    x.assign_add(a)\n",
    "    tf.print(x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=4.0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_print(tf.constant(3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面利用tf.Module的子类化将其封装一下。\n",
    "class DemoModule(tf.Module):\n",
    "    def __init__(self,init_value = tf.constant(0.0),name=None):\n",
    "        super(DemoModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #相当于with tf.name_scope(\"demo_module\")\n",
    "            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=True)\n",
    "\n",
    "     \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "    def addprint(self,a):\n",
    "        with self.name_scope:\n",
    "            self.x.assign_add(a)\n",
    "            tf.print(self.x)\n",
    "            return(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#执行\n",
    "demo = DemoModule(init_value = tf.constant(1.0))\n",
    "result = demo.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)\n",
      "(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)\n"
     ]
    }
   ],
   "source": [
    "#查看模块中的全部变量和全部可训练变量\n",
    "print(demo.variables)\n",
    "print(demo.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看模块中的全部子模块\n",
    "demo.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/demo/1/assets\n"
     ]
    }
   ],
   "source": [
    "#使用tf.saved_model 保存模型，并指定需要跨平台部署的方法\n",
    "tf.saved_model.save(demo,\"./data/demo/1\",signatures = {\"serving_default\":demo.addprint})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=11.0>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载模型\n",
    "demo2 = tf.saved_model.load(\"./data/demo/1\")\n",
    "demo2.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['a'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: ()\n",
      "        name: serving_default_a:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_0'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: ()\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: 'addprint'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          a: TensorSpec(shape=(), dtype=tf.float32, name='a')\n"
     ]
    }
   ],
   "source": [
    "# 查看模型文件相关信息，红框标出来的输出信息在模型部署和跨平台使用时有可能会用到\n",
    "!saved_model_cli show --dir ./data/demo/1 --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在tensorboard中查看计算图，模块会被添加模块名demo_module,方便层次化呈现计算图结构。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# 创建日志\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = './data/demomodule/%s' % stamp\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "#开启autograph跟踪\n",
    "tf.summary.trace_on(graph=True, profiler=True) \n",
    "\n",
    "#执行autograph\n",
    "demo = DemoModule(init_value = tf.constant(0.0))\n",
    "result = demo.addprint(tf.constant(5.0))\n",
    "\n",
    "#将计算图信息写入日志\n",
    "with writer.as_default():\n",
    "    tf.summary.trace_export(\n",
    "        name=\"demomodule\",\n",
    "        step=0,\n",
    "        profiler_outdir=logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#启动 tensorboard在jupyter中的魔法命令\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.start(\"--logdir ./data/demomodule/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了利用tf.Module的子类化实现封装，我们也可以通过给tf.Module添加属性的方法进行封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodule = tf.Module()\n",
    "mymodule.x = tf.Variable(0.0)\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  \n",
    "def addprint(a):\n",
    "    mymodule.x.assign_add(a)\n",
    "    tf.print(mymodule.x)\n",
    "    return (mymodule.x)\n",
    "\n",
    "mymodule.addprint = addprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodule.addprint(tf.constant(1.0)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,)\n"
     ]
    }
   ],
   "source": [
    "print(mymodule.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./data/mymodule/assets\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用tf.saved_model 保存模型\n",
    "tf.saved_model.save(mymodule,\"./data/mymodule\",\n",
    "    signatures = {\"serving_default\":mymodule.addprint})\n",
    "\n",
    "#加载模型\n",
    "mymodule2 = tf.saved_model.load(\"./data/mymodule\")\n",
    "mymodule2.addprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.Module和tf.keras.Model，tf.keras.layers.Layer\n",
    "\n",
    "\n",
    "tf.keras中的模型和层都是继承tf.Module实现的，也具有变量管理和子模块管理功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models,layers,losses,metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(issubclass(tf.keras.Model,tf.Module))\n",
    "print(issubclass(tf.keras.layers.Layer,tf.Module))\n",
    "print(issubclass(tf.keras.Model,tf.keras.layers.Layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4)                 44        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 57\n",
      "Trainable params: 57\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session() \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(4,input_shape = (10,)))\n",
    "model.add(layers.Dense(2))\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(10, 4) dtype=float32, numpy=\n",
       " array([[ 0.4522382 ,  0.43221498,  0.14367014, -0.5025284 ],\n",
       "        [ 0.12891138, -0.31865057, -0.65055746, -0.01515818],\n",
       "        [ 0.602229  ,  0.4051596 ,  0.29578292,  0.14352232],\n",
       "        [-0.48437193,  0.29420084,  0.24442518,  0.5773449 ],\n",
       "        [-0.1754905 ,  0.5620637 ,  0.05065989, -0.5278463 ],\n",
       "        [ 0.5395566 ,  0.3917793 ,  0.5911381 ,  0.21255642],\n",
       "        [ 0.08886194,  0.33382827,  0.17396307, -0.48383546],\n",
       "        [-0.4502645 , -0.39933273,  0.18566376,  0.63176715],\n",
       "        [ 0.15486294, -0.3905878 ,  0.40694213, -0.09025639],\n",
       "        [-0.30135456, -0.56823176,  0.1614539 ,  0.49237657]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[-0.9005773 , -0.75084114],\n",
       "        [ 0.22216773, -0.37016273],\n",
       "        [-0.28665853, -0.2095356 ],\n",
       "        [ 0.43346715, -0.1861248 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[-1.1895361 ],\n",
       "        [ 0.54658866]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
       " array([[-0.9005773 , -0.75084114],\n",
       "        [ 0.22216773, -0.37016273],\n",
       "        [-0.28665853, -0.2095356 ],\n",
       "        [ 0.43346715, -0.1861248 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[-1.1895361 ],\n",
       "        [ 0.54658866]], dtype=float32)>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].trainable = False #冻结第0层的变量,使其不可训练\n",
    "model.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f35bade4ef0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f35bade4ba8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f35bae22080>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f35bae220f0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.submodules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x7f35bade4ba8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f35bae22080>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f35bae220f0>]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential\n",
      "sequential\n"
     ]
    }
   ],
   "source": [
    "print(model.name)\n",
    "print(model.name_scope())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
